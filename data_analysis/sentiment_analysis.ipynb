{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from datetime import datetime, timezone\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import emoji\n",
    "import warnings\n",
    "import logging\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"tqdm\")\n",
    "import urllib3\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
   ],
   "id": "3cffd528c77b972d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.160163Z",
     "start_time": "2025-05-19T04:07:14.915647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ES_HOST = \"https://127.0.0.1:9200\"\n",
    "SOURCE_INDEX = \"moutputdata\"\n",
    "DEST_INDEX = \"sentiment_status\"\n",
    "auth = ('elastic', 'elastic')\n",
    "headers = {'Content-Type': 'application/json'}"
   ],
   "id": "68eb61dd2f0fe88",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.162292Z",
     "start_time": "2025-05-19T04:07:14.931341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"joeddav/xlm-roberta-large-xnli\",\n",
    "                      device=\"cpu\")\n",
    "\n",
    "labels = [\"support tariff\", \"oppose tariff\", \"neutral\"]"
   ],
   "id": "8212a4b5304b17c9",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.162520Z",
     "start_time": "2025-05-19T04:07:16.375999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def text_preprocessing(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # remove Markdown images and links\n",
    "    text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    text = re.sub(r'\\[.*?\\]\\(.*?\\)', '', text)\n",
    "    # keep hashtag words, remove the \"#\" symbol\n",
    "    text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    # remove @someone\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # convert emojis to text descriptions\n",
    "    text = emoji.demojize(text)\n",
    "    # normalize line breaks\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    # convert multiple whitespace into one space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove extra whitespace from start and end of text\n",
    "    return text.strip()"
   ],
   "id": "c9527fc68c8c97d1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.162944Z",
     "start_time": "2025-05-19T04:07:16.385121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_incremental_data(last_time, now):\n",
    "    query = {\n",
    "        \"query\": {\n",
    "            \"range\": {\n",
    "                \"metadata.created_time\": {\n",
    "                    \"gte\": last_time,\n",
    "                    \"lt\": now\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"size\": 1000,\n",
    "        \"sort\": [\"_doc\"]\n",
    "    }\n",
    "\n",
    "    url = f\"{ES_HOST}/{SOURCE_INDEX}/_search?scroll=2m\"\n",
    "    response = requests.post(url, headers=headers, auth=auth, json=query, verify=False).json()\n",
    "\n",
    "    scroll_id = response.get(\"_scroll_id\")\n",
    "    all_docs = response[\"hits\"][\"hits\"]\n",
    "\n",
    "    while scroll_id:\n",
    "        scroll_payload = {\"scroll\": \"2m\", \"scroll_id\": scroll_id}\n",
    "        scroll_response = requests.post(f\"{ES_HOST}/_search/scroll\", headers=headers, auth=auth, json=scroll_payload, verify=False).json()\n",
    "        hits = scroll_response[\"hits\"][\"hits\"]\n",
    "        if not hits:\n",
    "            break\n",
    "        all_docs.extend(hits)\n",
    "        scroll_id = scroll_response.get(\"_scroll_id\")\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "def doc_exists(index, doc_id):\n",
    "    url = f\"{ES_HOST}/{index}/_doc/{doc_id}\"\n",
    "    resp = requests.head(url, auth=auth, headers=headers, verify=False)\n",
    "    return resp.status_code == 200"
   ],
   "id": "f10eb4b7fbbd1e6b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.163836Z",
     "start_time": "2025-05-19T04:07:16.394256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_sentiment_pipeline():\n",
    "\n",
    "    try:\n",
    "        with open(\"last_time.txt\", \"r\") as f:\n",
    "            last_time = f.read().strip()\n",
    "    except FileNotFoundError:\n",
    "        last_time = \"1900-01-01T00:00:00Z\"\n",
    "\n",
    "    now = datetime.now(timezone.utc).isoformat().replace('+00:00', 'Z')\n",
    "    docs = get_incremental_data(last_time, now)\n",
    "\n",
    "    for item in tqdm(docs):\n",
    "\n",
    "        _id = item[\"_id\"]\n",
    "\n",
    "        if doc_exists(DEST_INDEX, _id):\n",
    "            continue\n",
    "\n",
    "        source = item.get(\"_source\", {})\n",
    "        text = source.get(\"content\", {}).get(\"text\", \"\")\n",
    "        cleaned = text_preprocessing(text)\n",
    "\n",
    "        if not cleaned:\n",
    "            sentiment = {\n",
    "                \"label\": None,\n",
    "                \"score\": None,\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            result = classifier(cleaned, candidate_labels=labels)\n",
    "            sentiment = {\n",
    "                \"label\": result[\"labels\"][0],\n",
    "                \"score\": round(result[\"scores\"][0], 6),\n",
    "            }\n",
    "\n",
    "        body = {\n",
    "            \"sentiment\": sentiment,\n",
    "            \"processed_at\": now\n",
    "        }\n",
    "\n",
    "        insert_url = f\"{ES_HOST}/{DEST_INDEX}/_doc/{_id}\"\n",
    "        resp = requests.put(insert_url, json=body, auth=auth, headers=headers, verify=False)\n",
    "\n",
    "        if resp.status_code not in [200, 201]:\n",
    "            print(f\"Insert failed for id {_id}: Status code {resp.status_code}, Response: {resp.text}\")\n",
    "\n",
    "    with open(\"last_time.txt\", \"w\") as f:\n",
    "        f.write(now)"
   ],
   "id": "a162e47eb4aace5f",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T04:46:55.163963Z",
     "start_time": "2025-05-19T04:07:16.403950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_sentiment_pipeline()"
   ],
   "id": "ea8653d3da9d2ff0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
